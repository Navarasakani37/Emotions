# -*- coding: utf-8 -*-
"""NM Emotions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IXFUb9hdMIpmnunyn6unL2j_vgyA--jW
"""

# Step 1: Importing Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import nltk
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix

# Step 2: Install necessary packages


# Step 3: Load the dataset
url = "sentiment.csv"
df = pd.read_csv(url)


# Step 4: Basic Exploration
print("Initial Shape:", df.shape)
df = df[['Text', 'Sentiment']]  # Selecting only relevant columns
df.dropna(inplace=True)  # Remove rows with missing values
df.drop_duplicates(inplace=True)

# Step 5: Text Cleaning
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = text.lower()  # Convert text to lowercase
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)  # Remove URLs
    text = re.sub(r"[^a-zA-Z\s]", '', text)  # Remove special characters and punctuation
    text = ' '.join(word for word in text.split() if word not in stop_words)  # Remove stopwords
    return text

# Apply cleaning function
df['Clean_Text'] = df['Text'].apply(clean_text)

# Remove emojis (non-ASCII characters)
df['Clean_Text'] = df['Clean_Text'].apply(lambda x: re.sub(r'[^\x00-\x7F]+', '', x))  # Remove emojis

# Step 6: Encode Sentiment Labels
label_encoder = LabelEncoder()
df['Encoded_Sentiment'] = label_encoder.fit_transform(df['Sentiment'])

# Step 7: Preview Cleaned Data
print("Cleaned Shape:", df.shape)
df.head()

# Step 2: Cleaned & Focused EDA

# ‚úÖ Step 1: Limit to Top 4 Emotions
top_emotions = df['Sentiment'].value_counts().nlargest(4).index
df_top = df[df['Sentiment'].isin(top_emotions)].copy()

# ‚úÖ Step 2: Extra stopwords cleanup for nicer word clouds
extra_stopwords = set(["im", "dont", "get", "got", "like", "would", "us"])
from nltk.corpus import stopwords
all_stopwords = set(stopwords.words('english')).union(extra_stopwords)

df_top['Clean_Text'] = df_top['Clean_Text'].apply(
    lambda x: ' '.join([word for word in x.split() if word not in all_stopwords])
)

# ‚úÖ Step 3: Better Count Plot
plt.figure(figsize=(10, 6))
sns.countplot(data=df_top, x='Sentiment', palette='Set2', order=top_emotions)
plt.title('Top 4 Emotion Distribution', fontsize=16)
plt.xlabel('Emotion', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# ‚úÖ Step 4: Box Plot for Text Length by Emotion
df_top['Text_Length'] = df_top['Clean_Text'].apply(lambda x: len(x.split()))
plt.figure(figsize=(10, 6))
sns.boxplot(data=df_top, x='Sentiment', y='Text_Length', palette='Pastel1')
plt.title('Text Length by Emotion', fontsize=16)
plt.xlabel('Emotion')
plt.ylabel('Number of Words')
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# ‚úÖ Step 5: Word Clouds for Each Emotion
from wordcloud import STOPWORDS, WordCloud

# Choose a color map (you can replace 'coolwarm' with other color maps such as 'Blues', 'Reds', etc.)
for emotion in top_emotions:
    text = " ".join(df_top[df_top['Sentiment'] == emotion]['Clean_Text'])
    wordcloud = WordCloud(
        width=1000,
        height=500,
        background_color='white',
        stopwords=STOPWORDS,
        colormap='coolwarm',  # Change colormap as desired
        max_words=100,
        max_font_size=100,
        min_font_size=10,
        prefer_horizontal=0.9,  # Make words more horizontal
        contour_width=3,
        contour_color='black'
    ).generate(text)

    plt.figure(figsize=(12, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Word Cloud for {emotion}', fontsize=16)
    plt.show()

# Step 3: Feature Engineering + TF-IDF

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# ‚úÖ TF-IDF Vectorization
tfidf = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed
X = tfidf.fit_transform(df_top['Clean_Text']).toarray()  # Input features
y = df_top['Encoded_Sentiment']  # Target labels (already encoded)

# ‚úÖ Optional: Check shape
print("TF-IDF Matrix Shape:", X.shape)
print("Target Shape:", y.shape)

# ‚úÖ Train-Test Split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

print("Training Set:", X_train.shape, y_train.shape)
print("Testing Set:", X_test.shape, y_test.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns
import matplotlib.pyplot as plt

# ‚úÖ Model 1: Logistic Regression
log_model = LogisticRegression(max_iter=1000)
log_model.fit(X_train, y_train)
y_pred_log = log_model.predict(X_test)

print("üîç Logistic Regression Performance:")
print(classification_report(y_test, y_pred_log, target_names=label_encoder.inverse_transform(sorted(y.unique()))))

# Confusion Matrix - Logistic Regression
cm_log = confusion_matrix(y_test, y_pred_log)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_log, display_labels=label_encoder.inverse_transform(sorted(y.unique())))
disp.plot(cmap='Blues')
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

# ‚úÖ Model 2: Multinomial Naive Bayes
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)
y_pred_nb = nb_model.predict(X_test)

print("üîç Naive Bayes Performance:")
print(classification_report(y_test, y_pred_nb, target_names=label_encoder.inverse_transform(sorted(y.unique()))))

# Confusion Matrix - Naive Bayes
cm_nb = confusion_matrix(y_test, y_pred_nb)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_nb, display_labels=label_encoder.inverse_transform(sorted(y.unique())))
disp.plot(cmap='Oranges')
plt.title("Confusion Matrix - Naive Bayes")
plt.show()

import numpy as np

# ‚úÖ Step 5A: Bar Chart Comparison of Metrics
from sklearn.metrics import precision_recall_fscore_support

# Get metrics
metrics_log = precision_recall_fscore_support(y_test, y_pred_log, average=None)
metrics_nb = precision_recall_fscore_support(y_test, y_pred_nb, average=None)
labels = label_encoder.inverse_transform(sorted(y.unique()))

# Prepare DataFrame
metrics_df = pd.DataFrame({
    'Emotion': labels,
    'Precision_LogReg': metrics_log[0],
    'Recall_LogReg': metrics_log[1],
    'F1_LogReg': metrics_log[2],
    'Precision_NB': metrics_nb[0],
    'Recall_NB': metrics_nb[1],
    'F1_NB': metrics_nb[2]
})

# Plotting
metrics_df.set_index('Emotion')[['F1_LogReg', 'F1_NB']].plot(kind='bar', figsize=(10,6), colormap='Set2')
plt.title("üìä F1-Score Comparison: Logistic Regression vs Naive Bayes")
plt.ylabel("F1-Score")
plt.xlabel("Emotion")
plt.xticks(rotation=45)
plt.ylim(0, 1.05)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# ‚úÖ Step 5B: Most Important Words (Logistic Regression)
feature_names = tfidf.get_feature_names_out()
coefs = log_model.coef_

for idx, emotion in enumerate(labels):
    top_n = 10
    top_words_idx = np.argsort(coefs[idx])[-top_n:]
    top_words = feature_names[top_words_idx]
    top_values = coefs[idx][top_words_idx]

    plt.figure(figsize=(8, 4))
    sns.barplot(x=top_values, y=top_words, palette='coolwarm')
    plt.title(f"üîç Top {top_n} Words Predicting: {emotion}")
    plt.xlabel("Coefficient Weight")
    plt.ylabel("Word")
    plt.tight_layout()
    plt.show()

!pip install -r requirements.txt
# Create and write to requirements.txt
with open("requirements.txt", "w") as f:
    f.write("""streamlit
pandas
numpy
matplotlib
seaborn
scikit-learn
nltk
wordcloud
""")

# Download to your computer
import streamlit as st
import pandas as pd

# Add the Streamlit file uploader widget
uploaded_file = st.file_uploader("sentiment", type=["csv"])

if uploaded_file is not None:
    # Read the uploaded CSV file
    df = pd.read_csv(uploaded_file) # Changed 'sentiment' to 'uploaded_file'
    # You can display the dataframe in the app if needed
    st.write(df.head())